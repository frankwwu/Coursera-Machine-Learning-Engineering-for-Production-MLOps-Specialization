# Knowledge Distillation

## Instructions

During this lab you are going to perform a model compression technique known as knowledge distillation.

You will code a custom class to implement the logic for the distillation process and use it to distill knowledge from a teacher model onto a student one.

[Launch Colab!](https://colab.research.google.com/drive/13rcQGTL-MtfrJ6Sbnh6kj36tDHXYUDpS)